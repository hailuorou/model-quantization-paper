# model-quantization-paper
reading list for quantization

## vision transformer model quantization
### PTQ(post training quantization)
- [(NeurIPS2021) Post-Training Quantization for Vision Transformer](https://arxiv.org/abs/2106.14156)
- [(IJCAI2022) FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer](https://arxiv.org/abs/2111.13824)
- [(ECCV2022) PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization](https://arxiv.org/abs/2111.12293)
- [(ACM MM2022) Towards Accurate Post-Training Quantization for Vision Transformer](https://arxiv.org/abs/2303.14341)
- [(ICASSAP2023) TSPTQ-ViT: TWO-SCALED POST-TRAINING QUANTIZATION FOR VISION TRANSFORMER](https://arxiv.org/ftp/arxiv/papers/2305/2305.12901.pdf)

### QAT
- [(NeurIPS2022) Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer](https://proceedings.neurips.cc/paper_files/paper/2022/hash/deb921bff461a7b0a5c344a4871e7101-Abstract-Conference.html)
- [Q-ViT: Fully Differentiable Quantization for Vision Transformer](https://arxiv.org/abs/2201.07703)
- [(TPAMI2023) Quantformer: Learning Extremely Low-Precision Vision Transformers](https://www.computer.org/csdl/journal/tp/2023/07/09992209/1JevxJcNsGc)
- [(AAAI2023) Quantized Feature Distillation for Network Quantization](https://arxiv.org/abs/2307.10638)